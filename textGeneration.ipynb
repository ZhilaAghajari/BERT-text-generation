{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39c6f15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json('train.json', lines = True)\n",
    "df_test = pd.read_json('test.json', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bd9a953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "      <th>rots</th>\n",
       "      <th>safety_label</th>\n",
       "      <th>safety_annotations</th>\n",
       "      <th>safety_annotation_reasons</th>\n",
       "      <th>source</th>\n",
       "      <th>etc</th>\n",
       "      <th>dialogue_id</th>\n",
       "      <th>response_id</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I tried a bit of shadowboxing today. Got one g...</td>\n",
       "      <td>This isn't a joke about black people, right? I...</td>\n",
       "      <td>[It's not right to think black people deserve ...</td>\n",
       "      <td>__needs_caution__</td>\n",
       "      <td>[needs caution, needs caution, needs caution]</td>\n",
       "      <td>[It's rude to make jokes about the color of pe...</td>\n",
       "      <td>sbic</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You're right, it isn't funny. Finding enjoymen...</td>\n",
       "      <td>I am glad that you agree. Joking about abusing...</td>\n",
       "      <td>[It's not right to think black people deserve ...</td>\n",
       "      <td>__casual__</td>\n",
       "      <td>[casual, casual, casual]</td>\n",
       "      <td>[The speaker admits they spoke inappropriately...</td>\n",
       "      <td>sbic</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Did I tell you how I used to poke holes in con...</td>\n",
       "      <td>That's a pretty low down thing to do.</td>\n",
       "      <td>[It's immoral to poke holes in condoms.]</td>\n",
       "      <td>__needs_intervention__</td>\n",
       "      <td>[needs caution, needs intervention, needs inte...</td>\n",
       "      <td>[Speaker admits to committing a seriously wron...</td>\n",
       "      <td>socialchemistry</td>\n",
       "      <td>reddit/confessions/bsajnv</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I wanted to prank others like that</td>\n",
       "      <td>You should not poke holes in condoms. You coul...</td>\n",
       "      <td>[It's harmful to tamper with other peoples con...</td>\n",
       "      <td>__needs_intervention__</td>\n",
       "      <td>[needs caution, needs intervention, needs inte...</td>\n",
       "      <td>[Speaker explains why they did the act., They ...</td>\n",
       "      <td>socialchemistry</td>\n",
       "      <td>reddit/confessions/bsajnv</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It's harmless. I doesn't hurt anyone. I just w...</td>\n",
       "      <td>You're not thinking. How do you conclude expos...</td>\n",
       "      <td>[It's normal for people to be alarmed by an un...</td>\n",
       "      <td>__needs_intervention__</td>\n",
       "      <td>[needs caution, needs intervention, needs inte...</td>\n",
       "      <td>[Speaker doesn't see why what they did was wro...</td>\n",
       "      <td>socialchemistry</td>\n",
       "      <td>reddit/confessions/bsajnv</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  I tried a bit of shadowboxing today. Got one g...   \n",
       "1  You're right, it isn't funny. Finding enjoymen...   \n",
       "2  Did I tell you how I used to poke holes in con...   \n",
       "3                 I wanted to prank others like that   \n",
       "4  It's harmless. I doesn't hurt anyone. I just w...   \n",
       "\n",
       "                                            response  \\\n",
       "0  This isn't a joke about black people, right? I...   \n",
       "1  I am glad that you agree. Joking about abusing...   \n",
       "2              That's a pretty low down thing to do.   \n",
       "3  You should not poke holes in condoms. You coul...   \n",
       "4  You're not thinking. How do you conclude expos...   \n",
       "\n",
       "                                                rots            safety_label  \\\n",
       "0  [It's not right to think black people deserve ...       __needs_caution__   \n",
       "1  [It's not right to think black people deserve ...              __casual__   \n",
       "2           [It's immoral to poke holes in condoms.]  __needs_intervention__   \n",
       "3  [It's harmful to tamper with other peoples con...  __needs_intervention__   \n",
       "4  [It's normal for people to be alarmed by an un...  __needs_intervention__   \n",
       "\n",
       "                                  safety_annotations  \\\n",
       "0      [needs caution, needs caution, needs caution]   \n",
       "1                           [casual, casual, casual]   \n",
       "2  [needs caution, needs intervention, needs inte...   \n",
       "3  [needs caution, needs intervention, needs inte...   \n",
       "4  [needs caution, needs intervention, needs inte...   \n",
       "\n",
       "                           safety_annotation_reasons           source  \\\n",
       "0  [It's rude to make jokes about the color of pe...             sbic   \n",
       "1  [The speaker admits they spoke inappropriately...             sbic   \n",
       "2  [Speaker admits to committing a seriously wron...  socialchemistry   \n",
       "3  [Speaker explains why they did the act., They ...  socialchemistry   \n",
       "4  [Speaker doesn't see why what they did was wro...  socialchemistry   \n",
       "\n",
       "                         etc  dialogue_id  response_id  episode_done  \n",
       "0                                       0            0         False  \n",
       "1                                       0            1          True  \n",
       "2  reddit/confessions/bsajnv            1            0         False  \n",
       "3  reddit/confessions/bsajnv            1            1         False  \n",
       "4  reddit/confessions/bsajnv            1            2          True  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "855bba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ca843f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I tried a bit of shadowboxing today. Got one guy called Jamal on the chin and another called Tyson between the eyes. This isn't a joke about black people, right? If it is, it isn't funny.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['context'].astype('str')+' '+df['response'].astype('str')\n",
    "df.iloc[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48f11792",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train set to train and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_input, val_input , train_labels, val_labels =  train_test_split (df, df['safety_label'] , test_size = .1, random_state = 123456789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec2da529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /opt/tljh/user/lib/python3.9/site-packages (4.24.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/tljh/user/lib/python3.9/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: filelock in /opt/tljh/user/lib/python3.9/site-packages (from transformers) (3.8.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/tljh/user/lib/python3.9/site-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: requests in /opt/tljh/user/lib/python3.9/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/tljh/user/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/tljh/user/lib/python3.9/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/tljh/user/lib/python3.9/site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/tljh/user/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jupyter-zha219/.local/lib/python3.9/site-packages (from transformers) (1.25.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/tljh/user/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/tljh/user/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/tljh/user/lib/python3.9/site-packages (from requests->transformers) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tljh/user/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/tljh/user/lib/python3.9/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/tljh/user/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77caafbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate in /home/jupyter-zha219/.local/lib/python3.9/site-packages (0.21.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/tljh/user/lib/python3.9/site-packages (from accelerate) (1.13.0.post200)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/tljh/user/lib/python3.9/site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: psutil in /opt/tljh/user/lib/python3.9/site-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /opt/tljh/user/lib/python3.9/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jupyter-zha219/.local/lib/python3.9/site-packages (from accelerate) (1.25.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/tljh/user/lib/python3.9/site-packages (from packaging>=20.0->accelerate) (3.0.9)\n",
      "Requirement already satisfied: typing_extensions in /opt/tljh/user/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (4.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff55d063",
   "metadata": {},
   "source": [
    "## Auto Tokenizer from transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68e9b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the input:\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "max_length = 128\n",
    "# def tokenize(element, max_length = 128):\n",
    "#   outputs = tokenizer(element['text'], truncation= True, max_length = max_length,\n",
    "#                       return_overflowing_tokens = True,return_length = True,)\n",
    "\n",
    "#   input_batch = []\n",
    "#   for length, input_ids in zip(outputs['length'], outputs['input_ids']):\n",
    "#     if length == max_length:\n",
    "#       input_batch.append(input_ids)\n",
    "#   return {\"input_ids\": input_batch}\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == 128:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "139d4f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bba23db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\":Dataset.from_pandas(train_input),\n",
    "    \"valid\": Dataset.from_pandas(val_input),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcd9dda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a8d280df476468d9fe83bb2df76d21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f01c9a9587d48e881e42fd4a64adcf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tokenize the train and validation sets:\n",
    "tokenized_dataset = raw_datasets.map(tokenize, batched= True, remove_columns=raw_datasets[\"train\"].column_names+['__index_level_0__'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3ab2404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108212, 108212, 108278, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_datasets['train']), len(train_input), len(tokenized_dataset['train']), len(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1495fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids'],\n",
       "    num_rows: 108278\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6657738b",
   "metadata": {},
   "source": [
    "## Build a model based on pre-trained text-generation. E.g., GPT2LMHeadModel, to compare the results against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aea7c0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoConfig\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=max_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "448505cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7f3712101a50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel(config)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1711bdb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 124.4M parameters\n"
     ]
    }
   ],
   "source": [
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09480581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a5f6e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create datacollator for train set and validation set\n",
    "batch_size = 16\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "train_dataloader = DataLoader(tokenized_dataset['train'], sampler = RandomSampler(tokenized_dataset['train']), batch_size = batch_size,collate_fn = data_collator)\n",
    "validation_dataloader = DataLoader(tokenized_dataset['valid'], sampler = RandomSampler(tokenized_dataset['valid']), batch_size = batch_size,collate_fn = data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d4ac574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4e719a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed_val = 12345678\n",
    "random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1ece19",
   "metadata": {},
   "source": [
    "## Fine-tune the model based on current dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02d4fd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/opt/tljh/user/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1, batch 100/6768, Loss: 0.24126736402511598\n",
      "Epoch 0/1, batch 200/6768, Loss: 0.1499951833486557\n",
      "Epoch 0/1, batch 300/6768, Loss: 0.12903139099478722\n",
      "Epoch 0/1, batch 400/6768, Loss: 0.12330042116343976\n",
      "Epoch 0/1, batch 500/6768, Loss: 0.11765031956136227\n",
      "Epoch 0/1, batch 600/6768, Loss: 0.11545532591640949\n",
      "Epoch 0/1, batch 700/6768, Loss: 0.11313444435596466\n",
      "Epoch 0/1, batch 800/6768, Loss: 0.11177447743713856\n",
      "Epoch 0/1, batch 900/6768, Loss: 0.1089200534671545\n",
      "Epoch 0/1, batch 1000/6768, Loss: 0.10869546681642532\n",
      "Epoch 0/1, batch 1100/6768, Loss: 0.10828962601721287\n",
      "Epoch 0/1, batch 1200/6768, Loss: 0.1059887908399105\n",
      "Epoch 0/1, batch 1300/6768, Loss: 0.10550639398396015\n",
      "Epoch 0/1, batch 1400/6768, Loss: 0.10557013876736164\n",
      "Epoch 0/1, batch 1500/6768, Loss: 0.10401114657521247\n",
      "Epoch 0/1, batch 1600/6768, Loss: 0.10481816731393337\n",
      "Epoch 0/1, batch 1700/6768, Loss: 0.10374127559363842\n",
      "Epoch 0/1, batch 1800/6768, Loss: 0.10372428111732006\n",
      "Epoch 0/1, batch 1900/6768, Loss: 0.0994092035293579\n",
      "Epoch 0/1, batch 2000/6768, Loss: 0.10077772460877896\n",
      "Epoch 0/1, batch 2100/6768, Loss: 0.10027714230120183\n",
      "Epoch 0/1, batch 2200/6768, Loss: 0.0979446879029274\n",
      "Epoch 0/1, batch 2300/6768, Loss: 0.09746638108044862\n",
      "Epoch 0/1, batch 2400/6768, Loss: 0.09980935789644718\n",
      "Epoch 0/1, batch 2500/6768, Loss: 0.09653687983751297\n",
      "Epoch 0/1, batch 2600/6768, Loss: 0.09727266758680343\n",
      "Epoch 0/1, batch 2700/6768, Loss: 0.0982626149058342\n",
      "Epoch 0/1, batch 2800/6768, Loss: 0.09878710806369781\n",
      "Epoch 0/1, batch 2900/6768, Loss: 0.09706065997481346\n",
      "Epoch 0/1, batch 3000/6768, Loss: 0.09752003706991673\n",
      "Epoch 0/1, batch 3100/6768, Loss: 0.09575804322957993\n",
      "Epoch 0/1, batch 3200/6768, Loss: 0.09602602273225784\n",
      "Epoch 0/1, batch 3300/6768, Loss: 0.09478112913668156\n",
      "Epoch 0/1, batch 3400/6768, Loss: 0.094014031291008\n",
      "Epoch 0/1, batch 3500/6768, Loss: 0.09377217903733254\n",
      "Epoch 0/1, batch 3600/6768, Loss: 0.09532822988927364\n",
      "Epoch 0/1, batch 3700/6768, Loss: 0.09385040372610093\n",
      "Epoch 0/1, batch 3800/6768, Loss: 0.09256534114480018\n",
      "Epoch 0/1, batch 3900/6768, Loss: 0.09426881566643715\n",
      "Epoch 0/1, batch 4000/6768, Loss: 0.09228078730404377\n",
      "Epoch 0/1, batch 4100/6768, Loss: 0.09236173368990422\n",
      "Epoch 0/1, batch 4200/6768, Loss: 0.09191243186593055\n",
      "Epoch 0/1, batch 4300/6768, Loss: 0.09147652566432952\n",
      "Epoch 0/1, batch 4400/6768, Loss: 0.09005995646119118\n",
      "Epoch 0/1, batch 4500/6768, Loss: 0.09340016424655914\n",
      "Epoch 0/1, batch 4600/6768, Loss: 0.09387911230325699\n",
      "Epoch 0/1, batch 4700/6768, Loss: 0.09228369750082493\n",
      "Epoch 0/1, batch 4800/6768, Loss: 0.09176992326974868\n",
      "Epoch 0/1, batch 4900/6768, Loss: 0.09338224902749062\n",
      "Epoch 0/1, batch 5000/6768, Loss: 0.09342433486133814\n",
      "Epoch 0/1, batch 5100/6768, Loss: 0.09071923494338989\n",
      "Epoch 0/1, batch 5200/6768, Loss: 0.0898604691401124\n",
      "Epoch 0/1, batch 5300/6768, Loss: 0.09151180319488049\n",
      "Epoch 0/1, batch 5400/6768, Loss: 0.08986760132014751\n",
      "Epoch 0/1, batch 5500/6768, Loss: 0.08996440935879946\n",
      "Epoch 0/1, batch 5600/6768, Loss: 0.09098383285105228\n",
      "Epoch 0/1, batch 5700/6768, Loss: 0.08879046723246574\n",
      "Epoch 0/1, batch 5800/6768, Loss: 0.08921822682023048\n",
      "Epoch 0/1, batch 5900/6768, Loss: 0.08995963219553232\n",
      "Epoch 0/1, batch 6000/6768, Loss: 0.08626264192163945\n",
      "Epoch 0/1, batch 6100/6768, Loss: 0.08956418171525002\n",
      "Epoch 0/1, batch 6200/6768, Loss: 0.08890205129981041\n",
      "Epoch 0/1, batch 6300/6768, Loss: 0.0897972172498703\n",
      "Epoch 0/1, batch 6400/6768, Loss: 0.08674066133797169\n",
      "Epoch 0/1, batch 6500/6768, Loss: 0.08846744872629643\n",
      "Epoch 0/1, batch 6600/6768, Loss: 0.08832218684256077\n",
      "Epoch 0/1, batch 6700/6768, Loss: 0.08835626356303691\n",
      "Epoch 0 finished with train loss of 0.10025321692228317 and validation loss of 0.0866947683030145\n",
      "Epoch 1/1, batch 100/6768, Loss: 0.08790471494197845\n",
      "Epoch 1/1, batch 200/6768, Loss: 0.08426163606345653\n",
      "Epoch 1/1, batch 300/6768, Loss: 0.08582509096711874\n",
      "Epoch 1/1, batch 400/6768, Loss: 0.08514936730265617\n",
      "Epoch 1/1, batch 500/6768, Loss: 0.08550475195050239\n",
      "Epoch 1/1, batch 600/6768, Loss: 0.08513401083648205\n",
      "Epoch 1/1, batch 700/6768, Loss: 0.08586869299411774\n",
      "Epoch 1/1, batch 800/6768, Loss: 0.08525794714689255\n",
      "Epoch 1/1, batch 900/6768, Loss: 0.08377835974097252\n",
      "Epoch 1/1, batch 1000/6768, Loss: 0.08495322480797768\n",
      "Epoch 1/1, batch 1100/6768, Loss: 0.08608027327805758\n",
      "Epoch 1/1, batch 1200/6768, Loss: 0.08537457298487425\n",
      "Epoch 1/1, batch 1300/6768, Loss: 0.08391906067728996\n",
      "Epoch 1/1, batch 1400/6768, Loss: 0.08660521574318408\n",
      "Epoch 1/1, batch 1500/6768, Loss: 0.08604691062122584\n",
      "Epoch 1/1, batch 1600/6768, Loss: 0.08508903048932552\n",
      "Epoch 1/1, batch 1700/6768, Loss: 0.08341761764138937\n",
      "Epoch 1/1, batch 1800/6768, Loss: 0.08481825590133667\n",
      "Epoch 1/1, batch 1900/6768, Loss: 0.08577719043940306\n",
      "Epoch 1/1, batch 2000/6768, Loss: 0.08313205160200596\n",
      "Epoch 1/1, batch 2100/6768, Loss: 0.08479409381747245\n",
      "Epoch 1/1, batch 2200/6768, Loss: 0.08343221046030522\n",
      "Epoch 1/1, batch 2300/6768, Loss: 0.08407498352229595\n",
      "Epoch 1/1, batch 2400/6768, Loss: 0.08255946364253759\n",
      "Epoch 1/1, batch 2500/6768, Loss: 0.08498725824058057\n",
      "Epoch 1/1, batch 2600/6768, Loss: 0.08270357966423035\n",
      "Epoch 1/1, batch 2700/6768, Loss: 0.08439856871962548\n",
      "Epoch 1/1, batch 2800/6768, Loss: 0.08440570052713156\n",
      "Epoch 1/1, batch 2900/6768, Loss: 0.08417729366570711\n",
      "Epoch 1/1, batch 3000/6768, Loss: 0.08300401877611875\n",
      "Epoch 1/1, batch 3100/6768, Loss: 0.08174089267849922\n",
      "Epoch 1/1, batch 3200/6768, Loss: 0.08181162543594837\n",
      "Epoch 1/1, batch 3300/6768, Loss: 0.08320909477770329\n",
      "Epoch 1/1, batch 3400/6768, Loss: 0.08350406885147095\n",
      "Epoch 1/1, batch 3500/6768, Loss: 0.0833228238299489\n",
      "Epoch 1/1, batch 3600/6768, Loss: 0.0836488687619567\n",
      "Epoch 1/1, batch 3700/6768, Loss: 0.08330827221274376\n",
      "Epoch 1/1, batch 3800/6768, Loss: 0.08406576324254274\n",
      "Epoch 1/1, batch 3900/6768, Loss: 0.083910819478333\n",
      "Epoch 1/1, batch 4000/6768, Loss: 0.08277810603380203\n",
      "Epoch 1/1, batch 4100/6768, Loss: 0.08208405829966069\n",
      "Epoch 1/1, batch 4200/6768, Loss: 0.08391520615667104\n",
      "Epoch 1/1, batch 4300/6768, Loss: 0.08495861515402794\n",
      "Epoch 1/1, batch 4400/6768, Loss: 0.08256377436220647\n",
      "Epoch 1/1, batch 4500/6768, Loss: 0.08353679779917002\n",
      "Epoch 1/1, batch 4600/6768, Loss: 0.08206645905971527\n",
      "Epoch 1/1, batch 4700/6768, Loss: 0.08176850903779269\n",
      "Epoch 1/1, batch 4800/6768, Loss: 0.08270898975431919\n",
      "Epoch 1/1, batch 4900/6768, Loss: 0.08186030346900225\n",
      "Epoch 1/1, batch 5000/6768, Loss: 0.0829989992082119\n",
      "Epoch 1/1, batch 5100/6768, Loss: 0.08058110609650612\n",
      "Epoch 1/1, batch 5200/6768, Loss: 0.08073098100721836\n",
      "Epoch 1/1, batch 5300/6768, Loss: 0.08215361598879099\n",
      "Epoch 1/1, batch 5400/6768, Loss: 0.08111227814108134\n",
      "Epoch 1/1, batch 5500/6768, Loss: 0.08202537387609482\n",
      "Epoch 1/1, batch 5600/6768, Loss: 0.08289459072053433\n",
      "Epoch 1/1, batch 5700/6768, Loss: 0.08287355616688728\n",
      "Epoch 1/1, batch 5800/6768, Loss: 0.08165900390595197\n",
      "Epoch 1/1, batch 5900/6768, Loss: 0.08182144299149513\n",
      "Epoch 1/1, batch 6000/6768, Loss: 0.08099492207169533\n",
      "Epoch 1/1, batch 6100/6768, Loss: 0.08161480732262134\n",
      "Epoch 1/1, batch 6200/6768, Loss: 0.08308756820857525\n",
      "Epoch 1/1, batch 6300/6768, Loss: 0.0808696373552084\n",
      "Epoch 1/1, batch 6400/6768, Loss: 0.08253285475075245\n",
      "Epoch 1/1, batch 6500/6768, Loss: 0.08175973556935787\n",
      "Epoch 1/1, batch 6600/6768, Loss: 0.08306597243994475\n",
      "Epoch 1/1, batch 6700/6768, Loss: 0.079931525811553\n",
      "Epoch 1 finished with train loss of 0.08342840522527695 and validation loss of 0.0812260095039977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50258, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50258, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fine-tuning the model\n",
    "import logging\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from accelerate import Accelerator\n",
    "\n",
    "#use CPU if the GPU is not avaiable\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "if not torch.cuda.is_available():\n",
    "  logging.warning('GPY is not available. The modeing is being trained on CPU!')\n",
    "\n",
    "\n",
    "'''max of epochs. Change this responsibily. Higher values for epochs can cause overfitting.\n",
    "'''\n",
    "MAX_EPOCH = 2\n",
    "\n",
    "warmup_steps = 1e2\n",
    "total_steps = len(train_dataloader)*MAX_EPOCH\n",
    "\n",
    "#preapre the model for training\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "BSIZE =batch_size\n",
    "#set the optimizer\n",
    "optim = AdamW(model.parameters(), lr=5e-5,eps = 1e-8)\n",
    "\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model, optim, train_dataloader, validation_dataloader)\n",
    "\n",
    "#set the criterion to compute error\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "#print the performance metrics over the training phase every iteration_reset=100\n",
    "iteration_reset = 100\n",
    "\n",
    "# This changes the learning rate as the training loop progresses\n",
    "scheduler = get_linear_schedule_with_warmup(optim,\n",
    "                                            num_warmup_steps = warmup_steps,\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "#main loop for epochs\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    loss_sum = 0.0\n",
    "\n",
    "    #batch training loop\n",
    "    for i,batch in enumerate(train_dataloader):\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        model.zero_grad() #?????\n",
    "        outputs = model(input_ids, labels = labels, attention_mask=attention_mask, token_type_ids=None)\n",
    "        # loss = outputs[0]\n",
    "        # loss = criterion(outputs.view(-1, tokenizer.vocab_size),labels.view(-1))\n",
    "        loss  = outputs[0]\n",
    "        loss_sum += loss.item()\n",
    "        train_loss += loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        #printing loss\n",
    "        if i% iteration_reset == 0 and i>0:\n",
    "            print('Epoch {0}/{1}, batch {2}/{3}, Loss: {4}'.format(epoch,MAX_EPOCH-1,i,len(train_dataloader),\n",
    "                                                                   loss_sum/(iteration_reset*BSIZE)))\n",
    "            loss_sum = 0\n",
    "    #compute model performance during training, and after each epoch:\n",
    "    model.eval() #put the model to the val phase; in other words, turn off the training phase for this model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for i, batch in enumerate(validation_dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, labels = labels, attention_mask = attention_mask)\n",
    "        loss = outputs[0].item()\n",
    "        val_loss +=loss\n",
    "    print('Epoch {0} finished with train loss of {1} and validation loss of {2}'.format(epoch, train_loss/(BSIZE*len(train_dataloader)), val_loss/(BSIZE*len(validation_dataloader))))\n",
    "    model.train() #put the model to the train phase..\n",
    "    model.train()\n",
    "\n",
    "\n",
    "model.eval()\n",
    "model.eval() #finally, we set off the training and put the model into validation for prediction use only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c60dad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained('gpt2-fine_tuned_26',from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbc7b527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'gpt2-fine_tuned_26_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "473558c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deccda82",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87eb50a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46d6b98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50258, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50258, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel(config)\n",
    "model.load_state_dict(torch.load('gpt2-fine_tuned_26_2'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c15c26a",
   "metadata": {},
   "source": [
    "## Check the model performance based on the test set: (Blue metrics and prexibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68e91d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_test = pd.read_json('test.json', lines = True)\n",
    "df_test['text'] = df_test['context'].astype('str')+' '+df_test['response'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3d74061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample = df_test.sample(frac = .005, random_state=123456, replace = False)\n",
    "len(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34b8a2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c876bbd609e84151b37249ab513e0d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tokenize the test set:\n",
    "from datasets import DatasetDict, Dataset\n",
    "test_datasets = DatasetDict({\n",
    "    \"test\":Dataset.from_pandas(test_sample),\n",
    "})\n",
    "#tokenize the train and validation sets:\n",
    "tokenized_test_dataset = test_datasets.map(tokenize, batched= True, remove_columns=test_datasets[\"test\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38bee3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "batch_size = 16\n",
    "test_dataloader = DataLoader(tokenized_test_dataset['test'], sampler = RandomSampler(tokenized_test_dataset['test']), batch_size = batch_size,collate_fn = data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e13fc8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test loss is : 0.24054135382175446\n",
      "42.88226538304938\n"
     ]
    }
   ],
   "source": [
    "# first, create the output for the test set:\n",
    "import numpy as np\n",
    "model.eval()\n",
    "test_loss =0.00\n",
    "perp = []\n",
    "for i , batch in enumerate(test_dataloader):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    model.zero_grad()\n",
    "    outputs = model(input_ids, labels = labels, attention_mask = attention_mask, token_type_ids = None)\n",
    "    loss = outputs[0]\n",
    "    test_loss+=loss\n",
    "    perp.append(loss.detach().to('cpu').tolist())\n",
    "print('total test loss is : {0}'.format(test_loss/len(tokenized_test_dataset['test'])))\n",
    "print(np.exp(np.mean(perp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fea7f50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['text'] = df_test['context']\n",
    "df_test['labels'] = df_test['response']\n",
    "df_test_sample = df_test.sample(frac = .005, random_state=123456, replace = False)\n",
    "len(df_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da6dc4c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1139531e83124e81b1d3a64cdecc9572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tokenize the test set:\n",
    "from datasets import DatasetDict, Dataset\n",
    "test_datasets = DatasetDict({\n",
    "    \"test\":Dataset.from_pandas(df_test_sample),\n",
    "})\n",
    "#tokenize the train and validation sets:\n",
    "tokenized_test_sample = test_datasets.map(tokenize, batched= True, remove_columns=test_datasets[\"test\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6c3d0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 25\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a641ba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, RandomSampler\n",
    "# batch_size = 16\n",
    "# test_sample_dataloader = DataLoader(tokenized_test_dataset['test'], sampler = RandomSampler(tokenized_test_sample['test']), batch_size = batch_size,collate_fn = data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a09f13e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: evaluate in /home/jupyter-zha219/.local/lib/python3.9/site-packages (0.4.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/tljh/user/lib/python3.9/site-packages (from evaluate) (2.26.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/tljh/user/lib/python3.9/site-packages (from evaluate) (2022.11.0)\n",
      "Requirement already satisfied: responses<0.19 in /opt/tljh/user/lib/python3.9/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/tljh/user/lib/python3.9/site-packages (from evaluate) (0.11.1)\n",
      "Requirement already satisfied: pandas in /opt/tljh/user/lib/python3.9/site-packages (from evaluate) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jupyter-zha219/.local/lib/python3.9/site-packages (from evaluate) (1.25.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/tljh/user/lib/python3.9/site-packages (from evaluate) (4.62.3)\n",
      "Requirement already satisfied: multiprocess in /opt/tljh/user/lib/python3.9/site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: packaging in /opt/tljh/user/lib/python3.9/site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: dill in /opt/tljh/user/lib/python3.9/site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: xxhash in /opt/tljh/user/lib/python3.9/site-packages (from evaluate) (0.0.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/tljh/user/lib/python3.9/site-packages (from evaluate) (2.7.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/tljh/user/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/tljh/user/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (10.0.1)\n",
      "Requirement already satisfied: aiohttp in /opt/tljh/user/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n",
      "Requirement already satisfied: filelock in /opt/tljh/user/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/tljh/user/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/tljh/user/lib/python3.9/site-packages (from packaging->evaluate) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/tljh/user/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/tljh/user/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tljh/user/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/tljh/user/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/tljh/user/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/tljh/user/lib/python3.9/site-packages (from pandas->evaluate) (2022.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/tljh/user/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/tljh/user/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/tljh/user/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/tljh/user/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/tljh/user/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/tljh/user/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/tljh/user/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf360bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b25cf00",
   "metadata": {},
   "source": [
    "### Get the Bleu and Perplexity metrics on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0900ace1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /opt/tljh/user/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.030849477664881848,\n",
       " 'precisions': [0.31100963977676305,\n",
       "  0.05796316359696641,\n",
       "  0.01336432306798373,\n",
       "  0.0037593984962406013],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.0035641547861507,\n",
       " 'translation_length': 1971,\n",
       " 'reference_length': 1964}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics.text import BLEUScore\n",
    "import evaluate\n",
    "\n",
    "bleu = evaluate.load('bleu')\n",
    "model.to(device)\n",
    "# bleu = BLEUScore()\n",
    "model.eval()\n",
    "res = 0.00\n",
    "ins = [];outs=[]\n",
    "for _,row in df_test_sample.iterrows():\n",
    "    output = model(tokenizer.encode(row.context,  return_tensors='pt').to(device))\n",
    "    out = tokenizer.decode(torch.argmax(output.logits, axis = 2)[0])\n",
    "    ins.append([row.context])\n",
    "    outs.append(out)\n",
    "#     res =bleu.compute(predictions=[out], references= [row[1].response],tokenizer = tokenizer)['bleu']\n",
    "\n",
    "# print('total bleu is  : {0}'.format(res))\n",
    "bleu.compute(predictions=outs, references= ins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aaad557f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.0,\n",
       " 'precisions': [0.005574912891986063, 0.0, 0.0, 0.0],\n",
       " 'brevity_penalty': 0.6916735972033735,\n",
       " 'length_ratio': 0.7306517311608961,\n",
       " 'translation_length': 1435,\n",
       " 'reference_length': 1964}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu.compute(predictions=outs, references= ins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93780120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchmetrics.text import Perplexity\n",
    "# import evaluate\n",
    "\n",
    "# prep = Perplexity\n",
    "# # model.to(device)\n",
    "# model.eval()\n",
    "# res = 0.00\n",
    "# ins, outs = [], [];\n",
    "# for _,row in df_test_sample.iterrows():\n",
    "#     output_tensor = model(tokenizer.encode(row.context,  return_tensors='pt').to(device))\n",
    "#     out = tokenizer.decode(torch.argmax(output.logits, axis = 2)[0])\n",
    "#     in_tensor = tokenizer.encode(row.context,  return_tensors='pt').to(device)\n",
    "#     ins.append([in_tensor])\n",
    "#     outs.append(out)\n",
    "# prep.update(ins, outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f95ae7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchmetrics.text import Perplexity\n",
    "# prep = Perplexity()\n",
    "# in_tensor = tokenizer.encode(row.context,  return_tensors='pt').to(device)\n",
    "# out_tensor = model(tokenizer.encode(row.context,  return_tensors='pt').to(device)).logits[0]\n",
    "# perp.update(in_tensor, out_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce93a470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(tokenizer.encode(row.context,  return_tensors='pt').to(device))\n",
    "type(model(tokenizer.encode(row.context,  return_tensors='pt').to(device)).logits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eb9700e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 51]), torch.Size([1, 15, 50258]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(row[1].response, return_tensors='pt').to(device).shape, output.logits.shape\n",
    "#PAS 12 va 17 chi hastan??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5069255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics using perprelxity :\n",
    "\n",
    "from torchmetrics.text import Perplexity\n",
    "perp = Perplexity(ignore_index=-100)\n",
    "res = 0.00\n",
    "for row in df_test_sample.iterrows():\n",
    "\n",
    "    output = model(tokenizer.encode(row[1].context,  return_tensors='pt').to(device))\n",
    "    \n",
    "    res +=perp(output.logits, tokenizer.encode(row[1].response, return_tensors='pt').to(device))\n",
    "\n",
    "print('total perp is  : {0}'.format(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6a0d90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [out], [[row[1].response]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ae2ef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting output for one single item \n",
    "input_id_sample = tokenizer.encode('this is a very nice and warm day outside.', return_tensors = 'pt')\n",
    "outputs = model(input_id_sample.to(device))\n",
    "output_sample = tokenizer.decode(torch.argmax(outputs.logits, axis = 2)[0])\n",
    "output_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d9433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a220f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d02b9890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from torchmetrics import BERTScore\n",
    "from torchmetrics.text import BLEUScore\n",
    "bleu = BLEUScore()\n",
    "preds = ['the cat is on the mat']\n",
    "target = [['a cat is on the mat']]\n",
    "\n",
    "bleu(preds, target)\n",
    "bleu = BLEUScore()\n",
    "bleu(['it is nice'], [['it is very nice']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "621126bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5661,  318,  257,  845, 3621,  290, 5814, 1110, 2354,   13]]),\n",
       " ' is a joke good thing I.. of I',\n",
       " tensor([[   40,   745, 19458,  ...,  -100,  -100,  -100],\n",
       "         [ 3347,  1297,   502,  ...,  -100,  -100,  -100],\n",
       "         [ 1544,   338,   616,  ...,  -100,  -100,  -100],\n",
       "         ...,\n",
       "         [11633,   314,  1683,  ...,  -100,  -100,  -100],\n",
       "         [   40,   836,   470,  ...,  -100,  -100,  -100],\n",
       "         [ 1026,   318,  1165,  ...,  -100,  -100,  -100]], device='cuda:0'))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_id_sample, output_sample, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd0770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "import bert_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0039bebb",
   "metadata": {},
   "source": [
    "### Generate output based on the tutorial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0568ab7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  27,   91, 9688, 1659, 5239,   91,   29]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "prompt = \"<|startoftext|>\"\n",
    "\n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
